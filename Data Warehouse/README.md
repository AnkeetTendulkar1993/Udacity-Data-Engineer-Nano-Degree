Data Warehouse for Music Streaming App

Purpose

The startup named Sparkify has a music streaming app. The user base and song database are growing at a very fast pace and there has been a need to move all their processes supporting the app to the Cloud. The purpose of this project is to make this data available in the AWS Cloud and make it available as dimensional tables for the analytics team to do analysis on this data to derive further insights from this data.  

Data Model

Fact Table

SongPlays
This table provides records with details of the songs played by the user.
Columns - songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent

Dimension Tables

Users
These are the users in the music app.
Columns - user_id, first_name, last_name, gender, level

Songs
This provides data about each song in the music database
Columns - song_id, title, artist_id, year, duration

Artists
This provides data each of the artists in music database
Columns - artist_id, name, location, lattitude, longitude

Time
This table has timestamps of records in songplays broken down into specific units
Columns - start_time, hour, day, week, month, year, weekday

Files

Input Data

The data folder consists of data used to populate the Dimension tables and Fact Table.

There are two datasets available as mentioned below.

[1] Song Dataset

The first dataset is a subset of real data from the Million Song Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. For example, here are filepaths to two files in this dataset.

song_data/A/B/C/TRABCEI128F424C983.json
song_data/A/A/B/TRAABJL12903CDCF1A.json

And below is an example of what a single song file, TRAABJL12903CDCF1A.json, looks like.

{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}

This data is used to populate the Songs and Artists table.

[2] Log Dataset

The second dataset consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations.

The log files in the dataset are partitioned by year and month. For example, here are filepaths to two files in this dataset.

log_data/2018/11/2018-11-12-events.json
log_data/2018/11/2018-11-13-events.json

The SongPlays, Users and Time tables are created with from data.


create_tables.py

This script consists of a python wrapper code used to drop existing tables and create new tables in the database.

etl.py

This is a python script used for loading data into the staging tables and executing the ETL script and loading the data into the target tables.

sql_queries.py

This script consists of all the SQL queries for creation of tables, loading data to staging tables in Redshift from S3 and transforming and loading data into the Fact and Dimension tables.


Execution

Open the terminal.

Execute the create_tables.py script to create the Fact and Dimension Tables.

root@aeb2c0d902da:/home/workspace# python create_tables.py 

Execute the etl.py script to load data into Redshift staging tables from S3, transform and load the data into the Fact and Dimension Tables.

root@aeb2c0d902da:/home/workspace# python etl.py 


Conclusion

Using this Star Schema queries can be written to for the purpose of analytics.

For example:

[1] Number of song plays for each artist in the month of December 2020.

SELECT s.artist_id, COUNT(*) 
FROM songplays s
INNER JOIN time t s.start_time = t.start_time
WHERE t.year = 2020 AND t.month = 12 
GROUP BY s.artist_id





